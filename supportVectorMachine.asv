function [nsv alpha bias] = NN_SVM(data,data_shuffled,C,p1,epsilon)
% NN_SVM: Support Vector Machine for Classification
%
%  Usage: [nsv alpha bias] = NN_SVM(X,Y,ker,C)
%
%  Parameters: X      - Training inputs
%              Y      - Training targets
%              ker    - kernel function
%              C      - upper bound (non-separable case)
%              nsv    - number of support vectors
%              alpha  - Lagrange Multipliers
%              b0     - bias term
%
%  Original code: Steve Gunn (srg@ecs.soton.ac.uk)
%  Modified by Yanbo Xue (yxue@soma.mcmaster.ca) for non-commercial use
%  March 28, 2007
% 
%--------------------- BEGIN OF MAIN ROUTINE -----------------------------

%============== Step 0A: Generate halfmoon data ==========================
% rad      = 10;   % central radius of the half moon
% width    = 6;    % width of the half moon
% dist     = -4;   % distance between two half moons
% num_tr   = 300;  % # of training datasets
% num_te   = 2000; % # of testing datasets
% num_samp = num_tr + num_te;% number of samples
% fprintf('Support Vector Machine for Classification\n');
% fprintf('_________________________________________\n');
% fprintf('Generating halfmoon data ...\n');
% fprintf('  ------------------------------------\n');
% fprintf('  Points generated: %d\n',num_samp);
% fprintf('  Halfmoon radius : %2.1f\n',rad);
% fprintf('  Halfmoon width  : %2.1f\n',width);
% fprintf('      Distance    : %2.1f\n',dist);
% fprintf('  ------------------------------------\n');
% seed=2e5;
% rand('seed',seed);
% [data, data_shuffled] = halfmoon(rad,width,dist,num_samp);
warning off;


%================== Step 0B: Initialization of SVM =======================
% C      = 50;  % upper bound (non-separable case)
% p1     = 1;    % sigma for RBF, width
% epsilon= 1e-5; % threshold for Support Vector Detection
err    = 0;    % a counter to denote the number of error outputs
b0     = 0;    % Implicit bias, 0 for RBF kernal

%%==== Step 1: Preprocess the input data, remove mean and normalize =======
mean0 = [mean(data(1:2,:)')';0];         % mean of the original data
for i = 1:num_samp
    data_norm(:,i) = data_shuffled(:,i) - mean0;
end
max0  = [max(abs(data_norm(1:2,:)'))';1];% max of the original data
for i = 1:num_samp
    data_norm(:,i) = data_norm(:,i)./max0;
end

%========== Step 2: Fetch Training and Testing Data ======================
X_tr = data_norm(1:2,1:num_tr)';
Y_tr = data_norm(3,1:num_tr)';
X_te = data_norm(1:2,1+num_tr:num_tr+num_te)';
Y_te = data_norm(3,1+num_tr:num_tr+num_te)';

%==================== Step 3: Construct the Kernel matrix =================
fprintf('Constructing RBF kernal matrix ...\n');
H = zeros(num_tr,num_tr);
for i=1:num_tr
    for j=1:num_tr
        H(i,j) = Y_tr(i)*Y_tr(j)*(exp(-(X_tr(i,:)-X_tr(j,:))*(X_tr(i,:)-X_tr(j,:))'/(2*p1^2)));
    end
end
c = -ones(num_tr,1);
H = H + 1E-10*randn(size(H)); % add artifical noise to avoid Hessian problem.

%============ Step 4: Use QP to solve the Optimization Problem ============
% Set up the parameters for the Optimization problem
fprintf('Optimizing using QP ...\n');
vlb = zeros(num_tr,1); % Set the bounds: alphas >= 0
vub = C*ones(num_tr,1);%                 alphas <= C
x0  = zeros(num_tr,1); % The starting point is [0 0 0   0]
neqcstr = 0;           % Set the number of equality constraints (1 or 0)
A = []; 
b = [];
% Running QP to solve the optimization problem
% The QP is implemented using C++ and then exported to a .dll file
st = cputime;
[alpha lambda how] = qp(H, c, A, b, vlb, vub, x0, neqcstr);

%======== Step 5: Compute the number of Support Vectors ===================
svi = find( alpha > epsilon);
nsv = length(svi);

fprintf('  ------------------------------------\n');
fprintf('   Points trained: %d\n',num_tr);
fprintf('   Execution time: %4.1f seconds\n',cputime - st);
fprintf('       Status    : %s\n',how);
fprintf('         C       : %f\n',C);
w2 = alpha'*H*alpha;
fprintf('       |w0|^2    : %f\n',w2);
fprintf('       Margin    : %f\n',2/sqrt(w2));
fprintf('      Sum alpha  : %f\n',sum(alpha));
fprintf('  Support Vectors: %d (%3.1f%%)\n',nsv,100*nsv/num_tr);
fprintf('  ------------------------------------\n');

%============= Step 6: Plot training result ===============================
figure;
svcplot_train(X_tr,Y_tr,alpha,b0,epsilon,p1,mean0,max0);
xlabel('x');ylabel('y');
title(['Classification using SVM with dist = ',num2str(dist),', radius = ',...
       num2str(rad), ' and width = ',num2str(width), '(Training)']);

%=============== Step 7: Test the trained SVM ============================


%================== Final: Output data for further process ================
% data.X_tr = X_tr; % training input
% data.Y_tr = Y_tr; % training output
% data.X_te = X_te; % testing input
% data.Y_te = Y_te; % ideal testing output
% data.Y_pred=Y_pred;% predicted testing output
% data.mean = mean0;% mean value of the data (training + testing)
% data.max  = max0; % max value of the data (training + testing)
% data.dist = dist; % parameter of halfmoon data: distance
% data.rad  = rad;  % parameter of halfmoon data: radius
% data.width= width;% parameter of halfmoon data: width
% data.err  = err;  % number of error outputs

